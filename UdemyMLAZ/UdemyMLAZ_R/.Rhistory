setwd("~/GitHub/ml_course-udemy/UdemyMLAZ")
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset
setwd("~/GitHub/ml_course-udemy/UdemyMLAZ/UdemyMLAZ_R")
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN=100)
rules = apriori(data = dataset, parameter = list(support = 0.03, confidence = 0.8))
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1")
install.packages(c("rj", "rj.gd"))
options(install.packages.check.source = "no")
install.packages(c("rj", "rj.gd"))
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1", dependencies = TRUE)
install.packages("devtools")
install.packages('rJava')
install.packages('rjava')
install.packages('rj')
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1", dependencies = TRUE)
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1", dependencies = TRUE, type='source')
library(rj)
library(rJava)
install.packages("D:\Downloads\rj_2.1.0-11.zip", repos=NULL, dependencies = TRUE, type='source')
install.packages("D:\\Downloads\\rj_2.1.0-11.zip", repos=NULL, dependencies = TRUE, type='source')
install.packages("D:\\Downloads\\rj.gd_2.1.0-2.zip", repos=NULL, dependencies = TRUE, type='source')
library(rj)
#install.packages('arules')
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=100)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.4))
#install.packages('arules')
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.4))
inspect(sort(rules, by = 'lift')[1:10])
inspect(sort(rules, by = 'lift')[1:3])
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.4))
inspect(sort(rules, by = 'lift')[1:3])
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.3))
inspect(sort(rules, by = 'lift')[1:3])
inspect(sort(rules, by = 'lift')[1:10])
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.003, confidence = 0.4))
inspect(sort(rules, by = 'lift')[1:10])
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.003, confidence = 0.2))
inspect(sort(rules, by = 'lift')[1:10])
inspect(sort(rules, by = list('lift','confidence'))[1:10])
inspect(sort(rules, by = 'lift')[1:10])
inspect(sort(rules, by = c('lift','confidence'))[1:10])
inspect(sort(rules, by = 'lift')[1:10])
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.004, confidence = 0.2))
#visualization of rules
inspect(sort(rules, by = 'lift')[1:10])
dataset = read.transactions("..\\AssociationRulesLearning_Eclat\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
library(arules)
dataset = read.transactions("..\\AssociationRulesLearning_Eclat\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
dataset = read.transactions("..\\AssociationRulesLearning_Eclat\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN=10)
rules = eclat(data = dataset, parameter = list(support = 0.004, minlen = 2))
rules
inspect(sort(rules, by = 'support')[1:10])
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv.csv", header = TRUE)
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = TRUE)
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = TRUE)
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = FALSE)
size(dataset)
len
length(dataset)
count(dataset)
dataset.nrow
nrow(dataset)
a = 0
a
a = integer(0)
a
z = 1e400 z
z
2^2
10^400
aa = integer(0)
aa[1] += 1
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = FALSE)
#implement UCB
N = nrow(dataset) - 1 #remove the header row from the count
d = length(dataset) #how many ads
ads_selected = integer(0)
count_of_selection = integer(d)
sums_of_rewards = 0
reward = 0
total_reward = 0
for (n in 1:N) {
max_upper_bound = 0
ad = 0
for (i in 1:d) {
upper_bound = 0
if (count_of_selection[i] > 0) {
average_reward = sums_of_rewards[i] / count_of_selection[i]
delta_i = sqrt(3 / 2 * log(n) / count_of_selection[i])
upper_bound = average_reward + delta_i
} else {
upper_bound =  10^400
}
if (upper_bound > max_upper_bound) {
max_upper_bound = upper_bound
ad = i
}
}
ads_selected = append(ads_selected, ad)
count_of_selection[ad] = count_of_selection[ad] + 1
reward = dataset[n, ad]
sums_of_rewards[ad] = sums_of_rewards[ad] + reward
total_reward = total_reward + reward
}
dataset = read.csv('Ads_CTR_Optimisation.csv')
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = FALSE)
#implement UCB
N = nrow(dataset) - 1 #remove the header row from the count
d = length(dataset) #how many ads
ads_selected = integer(0)
count_of_selection = integer(d)
sums_of_rewards = 0
reward = 0
total_reward = 0
for (n in 1:N) {
max_upper_bound = 0
ad = 0
for (i in 1:d) {
upper_bound = 0
if (count_of_selection[i] > 0) {
average_reward = sums_of_rewards[i] / count_of_selection[i]
delta_i = sqrt(3 / 2 * log(n) / count_of_selection[i])
upper_bound = average_reward + delta_i
} else {
upper_bound =  1000000
}
if (upper_bound > max_upper_bound) {
max_upper_bound = upper_bound
ad = i
}
}
ads_selected = append(ads_selected, ad)
count_of_selection[ad] = count_of_selection[ad] + 1
reward = dataset[n, ad]
sums_of_rewards[ad] = sums_of_rewards[ad] + reward
total_reward = total_reward + reward
}
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv", header = TRUE)
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv", header = TRUE, quote = '')
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringAsFactors = FALSE)
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus[0]
as.character(corpus[[841]])
install.packages('snowballC')
install.packages('SnowballC')
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
library(SnowballC)
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
as.character(corpus[[1]])
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers)
as.character(corpus[[1]])
corpus = tm_map(corpus, removePunctuation)
as.character(corpus[[1]])
corpus = tm_map(corpus, removeWords, stopwords())
as.character(corpus[[1]])
corpus = tm_map(corpus, stemDocument)
as.character(corpus[[1]])
cat('to lower: ', as.character(corpus[[1]]))
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]))
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
print('to lower: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeNumbers)
print('remove numbers: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removePunctuation)
print('remove punctuation:', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeWords, stopwords())
print('remove stop words: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, stemDocument)
print('stem document: ', as.character(corpus[[1]]))
print('to lower: '+ as.character(corpus[[1]]))
writeLines('to lower: '+ as.character(corpus[[1]]))
writeLines('to lower: '+ as.character(corpus
cat('stem document: ', as.character(corpus[[1]]),'\n')[[1]]))
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]),'\n')
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]),'\n')
corpus = tm_map(corpus, stripWhitespace)
cat('stem document: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, stripWhitespace)
cat('stem document: ', as.character(corpus[[841]]), '\n')
dtm = DocumentTermMatrix(corpus)
library(SnowballC)
dtm = DocumentTermMatrix(corpus)
dtm
library(SnowballC)
dtm = DocumentTermMatrix(corpus)
dtm
dtm = removeSparseTerms(dtm, 0.99)
dtm
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]),'\n')
corpus = tm_map(corpus, stripWhitespace)
cat('stem document: ', as.character(corpus[[841]]), '\n')
#install.packages('SnowballC')
# build sparse matrix
library(SnowballC)
dtm = DocumentTermMatrix(corpus)
dtm
dtm = removeSparseTerms(dtm, 0.999)
dtm
type(dtm)
dtm
# create X,y from the dataset, build dataframe
dataset = as.data.frame(as.matrix(dtm))
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
dependentVariable_Liked = dataset$Liked
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]),'\n')
corpus = tm_map(corpus, stripWhitespace)
cat('stem document: ', as.character(corpus[[841]]), '\n')
#install.packages('SnowballC')
# build sparse matrix
library(SnowballC)
dtm = DocumentTermMatrix(corpus)
dtm
#minimize matrix
dtm = removeSparseTerms(dtm, 0.999)
dtm
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dependentVariable_Liked
dataset$Liked
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
dependentVariable_Liked = dataset$Liked
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]),'\n')
corpus = tm_map(corpus, stripWhitespace)
cat('stem document: ', as.character(corpus[[841]]), '\n')
#install.packages('SnowballC')
# build sparse matrix
library(SnowballC)
dtm = DocumentTermMatrix(corpus)
dtm
#minimize matrix
dtm = removeSparseTerms(dtm, 0.999)
dtm
# create X,y from the dataset, build dataframe
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dependentVariable_Liked
# Encoding the target feature as factor dataset$Liked = factor(dataset$Liked, levels = c(0, 1)) dataset
# Splitting the dataset into the Training set and Test set # install.packages('caTools') library(caTools) set.seed(123) split = sample.split(dataset$Liked, SplitRatio = 0.8) training_set = subset(dataset, split == TRUE) test_set = subset(dataset, split == FALSE)
# Fitting classifier to the Training set # Create your classifier here library(randomForest) classifier = randomForest(x = training_set[-692],                           y = training_set$Liked,                           ntree = 10) # Predicting the Test set results y_pred = predict(classifier, newdata = test_set[-692]) y_pred test_set$Liked # Making the Confusion Matrix # cm = table(test_set[, 3], y_pred) cm = table(test_set$Liked, y_pred) cm
install.packages("rafalib")
install.packages("swirl")
swirl()
install.packages("swirl")
library(swirl)
swirl()
version
swirl()
swirl()
(0.5,55,-10,6)
num_vect <- (0.5,55,-10,6)
swirl()
5 + 7
x <- 5+7
x
y = x-3
y <- x-3
y
c(1.1,9.3.14)
c(1.1,9,3.14)
z <- c(1.1,9,3.14)
swirl()
swirl()
1:20
pi:10
15:1
?`:`
seq(1,20)
seq(1,10,by = 0.5)
seq(0,10,by = 0.5)
seq(5,10,length = 30)
my_seq<-seq(5,10,length = 30)
length(my_seq)
1:length(my_seq)
seq(along.with=my_seq)
seq_along(my_seq)
rep(0, times=40)
rep(c(0,1,2), times=10)
rep(c(0,1,2), each=10)
nums <- c(2.23,3.45,1.87,2.11,7.33,18.34,19.23)
ave(nums)
require(graphics)
ave(nums)
ave(nums)[1]
swirl()
swirl()
sum(1:25)
sum(seq(1,25)^2)
swirl()
c(0.5, 55, -10, 6)
num_vect<-c(0.5, 55, -10, 6)
tf <- num_vect < 1
tf
num_vect >= 6
c('My', 'name', 'is')
c("My", "name", "is")
my_char<-c("My", "name", "is")
my_char
paste(my_char, collapse = )
paste(my_char, collapse = " ")
c(my_char)
c(my_char, "Ahmed")
c(my_char, "AA")
my_name<-c(my_char, "AA")
my_name
paste(my_name, collapse = " ")
paste("Hello", "World", sep=" ")
paste("Hello", "World!", sep=" ")
class(my_name)
paste("Hello", "world!", sep = " ")
paste(1:3,c("X","Y","z"),sep = "")
paste(1:3, c("X","Y","z"), sep = "")
paste(1:3, c("X","Y","Z"), sep = "")
paste(LETTERS, 1:4, sep = "-")
cars
class(cars)
nrow(cars)
ave(cars[,2])
ave(cars[,2])[1]
which(cars$dist == 85)
