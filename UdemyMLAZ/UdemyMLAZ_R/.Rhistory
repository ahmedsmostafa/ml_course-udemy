getwd()
dir.create('OutputDataRAhmed')
2.2e-16 < 0.05
data(Boston, package="MASS")
myList <- list(1, "Hello", TRUE)
print(myList[1])
class(myList)
class(myList[1])
summation <- mylist[1]+3
summation <- myList[1]+3
summation <- myList[[1]]+3
print(summation)
myMatrix <- matrix(c(1:10), 2)
print(myMatrix)
myMatrix <- matrix(c(1:10), 3,4)
myMatrix <- matrix(c(1:10), 5,2)
print(myMatrix)
print(myMatrix(2,2))
print(myMatrix[2,2])
print(myMatrix[,2])
print(myMatrix[2])
print(myMatrix[2,1])
print(myMatrix[2,])
print(myMatrix[1,])
print(myMatrix[,1])
print(myMatrix[,2])
dimensions <- c("ROWS", "COLUMNS")
myMatrix.dimnames=list(dimensions)
print(myMatrix)
dimnames(myMatrix) <- list(dimensions)
dimnames(myMatrix) <- dimensions
dimensions <- c("ROWS", "COLUMN1", "COLUMN2")
dimnames(myMatrix) <- list(dimensions)
install.packages('e1071')
exit
install.packages('arules')
setwd("~/GitHub/ml_course-udemy")
setwd("~/GitHub/ml_course-udemy/UdemyMLAZ")
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset
setwd("~/GitHub/ml_course-udemy/UdemyMLAZ/UdemyMLAZ_R")
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN=100)
rules = apriori(data = dataset, parameter = list(support = 0.03, confidence = 0.8))
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1")
install.packages(c("rj", "rj.gd"))
options(install.packages.check.source = "no")
install.packages(c("rj", "rj.gd"))
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1", dependencies = TRUE)
install.packages("devtools")
install.packages('rJava')
install.packages('rjava')
install.packages('rj')
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1", dependencies = TRUE)
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1", dependencies = TRUE, type='source')
library(rj)
library(rJava)
install.packages("D:\Downloads\rj_2.1.0-11.zip", repos=NULL, dependencies = TRUE, type='source')
install.packages("D:\\Downloads\\rj_2.1.0-11.zip", repos=NULL, dependencies = TRUE, type='source')
install.packages("D:\\Downloads\\rj.gd_2.1.0-2.zip", repos=NULL, dependencies = TRUE, type='source')
library(rj)
#install.packages('arules')
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=100)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.4))
#install.packages('arules')
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.4))
inspect(sort(rules, by = 'lift')[1:10])
inspect(sort(rules, by = 'lift')[1:3])
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.4))
inspect(sort(rules, by = 'lift')[1:3])
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.3))
inspect(sort(rules, by = 'lift')[1:3])
inspect(sort(rules, by = 'lift')[1:10])
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.003, confidence = 0.4))
inspect(sort(rules, by = 'lift')[1:10])
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.003, confidence = 0.2))
inspect(sort(rules, by = 'lift')[1:10])
inspect(sort(rules, by = list('lift','confidence'))[1:10])
inspect(sort(rules, by = 'lift')[1:10])
inspect(sort(rules, by = c('lift','confidence'))[1:10])
inspect(sort(rules, by = 'lift')[1:10])
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.004, confidence = 0.2))
#visualization of rules
inspect(sort(rules, by = 'lift')[1:10])
dataset = read.transactions("..\\AssociationRulesLearning_Eclat\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
library(arules)
dataset = read.transactions("..\\AssociationRulesLearning_Eclat\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
dataset = read.transactions("..\\AssociationRulesLearning_Eclat\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN=10)
rules = eclat(data = dataset, parameter = list(support = 0.004, minlen = 2))
rules
inspect(sort(rules, by = 'support')[1:10])
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv.csv", header = TRUE)
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = TRUE)
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = TRUE)
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = FALSE)
size(dataset)
len
length(dataset)
count(dataset)
dataset.nrow
nrow(dataset)
a = 0
a
a = integer(0)
a
z = 1e400Â z
z
2^2
10^400
aa = integer(0)
aa[1] += 1
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = FALSE)
#implement UCB
N = nrow(dataset) - 1 #remove the header row from the count
d = length(dataset) #how many ads
ads_selected = integer(0)
count_of_selection = integer(d)
sums_of_rewards = 0
reward = 0
total_reward = 0
for (n in 1:N) {
    max_upper_bound = 0
    ad = 0
    for (i in 1:d) {
        upper_bound = 0
        if (count_of_selection[i] > 0) {
            average_reward = sums_of_rewards[i] / count_of_selection[i]
            delta_i = sqrt(3 / 2 * log(n) / count_of_selection[i])
            upper_bound = average_reward + delta_i
        } else {
            upper_bound =  10^400
        }
        if (upper_bound > max_upper_bound) {
            max_upper_bound = upper_bound
            ad = i
        }
    }
    ads_selected = append(ads_selected, ad)
    count_of_selection[ad] = count_of_selection[ad] + 1
    reward = dataset[n, ad]
    sums_of_rewards[ad] = sums_of_rewards[ad] + reward
    total_reward = total_reward + reward
}
dataset = read.csv('Ads_CTR_Optimisation.csv')
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = FALSE)
#implement UCB
N = nrow(dataset) - 1 #remove the header row from the count
d = length(dataset) #how many ads
ads_selected = integer(0)
count_of_selection = integer(d)
sums_of_rewards = 0
reward = 0
total_reward = 0
for (n in 1:N) {
    max_upper_bound = 0
    ad = 0
    for (i in 1:d) {
        upper_bound = 0
        if (count_of_selection[i] > 0) {
            average_reward = sums_of_rewards[i] / count_of_selection[i]
            delta_i = sqrt(3 / 2 * log(n) / count_of_selection[i])
            upper_bound = average_reward + delta_i
        } else {
            upper_bound =  1000000
        }
        if (upper_bound > max_upper_bound) {
            max_upper_bound = upper_bound
            ad = i
        }
    }
    ads_selected = append(ads_selected, ad)
    count_of_selection[ad] = count_of_selection[ad] + 1
    reward = dataset[n, ad]
    sums_of_rewards[ad] = sums_of_rewards[ad] + reward
    total_reward = total_reward + reward
}
