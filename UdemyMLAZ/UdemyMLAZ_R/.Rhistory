getwd()
dir.create('OutputDataRAhmed')
2.2e-16 < 0.05
data(Boston, package="MASS")
myList <- list(1, "Hello", TRUE)
print(myList[1])
class(myList)
class(myList[1])
summation <- mylist[1]+3
summation <- myList[1]+3
summation <- myList[[1]]+3
print(summation)
myMatrix <- matrix(c(1:10), 2)
print(myMatrix)
myMatrix <- matrix(c(1:10), 3,4)
myMatrix <- matrix(c(1:10), 5,2)
print(myMatrix)
print(myMatrix(2,2))
print(myMatrix[2,2])
print(myMatrix[,2])
print(myMatrix[2])
print(myMatrix[2,1])
print(myMatrix[2,])
print(myMatrix[1,])
print(myMatrix[,1])
print(myMatrix[,2])
dimensions <- c("ROWS", "COLUMNS")
myMatrix.dimnames=list(dimensions)
print(myMatrix)
dimnames(myMatrix) <- list(dimensions)
dimnames(myMatrix) <- dimensions
dimensions <- c("ROWS", "COLUMN1", "COLUMN2")
dimnames(myMatrix) <- list(dimensions)
install.packages('e1071')
exit
install.packages('arules')
setwd("~/GitHub/ml_course-udemy")
setwd("~/GitHub/ml_course-udemy/UdemyMLAZ")
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset
setwd("~/GitHub/ml_course-udemy/UdemyMLAZ/UdemyMLAZ_R")
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN=100)
rules = apriori(data = dataset, parameter = list(support = 0.03, confidence = 0.8))
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1")
install.packages(c("rj", "rj.gd"))
options(install.packages.check.source = "no")
install.packages(c("rj", "rj.gd"))
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1", dependencies = TRUE)
install.packages("devtools")
install.packages('rJava')
install.packages('rjava')
install.packages('rj')
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1", dependencies = TRUE)
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-2.1", dependencies = TRUE, type='source')
library(rj)
library(rJava)
install.packages("D:\Downloads\rj_2.1.0-11.zip", repos=NULL, dependencies = TRUE, type='source')
install.packages("D:\\Downloads\\rj_2.1.0-11.zip", repos=NULL, dependencies = TRUE, type='source')
install.packages("D:\\Downloads\\rj.gd_2.1.0-2.zip", repos=NULL, dependencies = TRUE, type='source')
library(rj)
#install.packages('arules')
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=100)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.4))
#install.packages('arules')
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.4))
inspect(sort(rules, by = 'lift')[1:10])
inspect(sort(rules, by = 'lift')[1:3])
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.4))
inspect(sort(rules, by = 'lift')[1:3])
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.02, confidence = 0.3))
inspect(sort(rules, by = 'lift')[1:3])
inspect(sort(rules, by = 'lift')[1:10])
library(arules)
dataset = read.csv("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", header = FALSE)
#use read transactions to build a sparse matrix
dataset = read.transactions("..\\AssociationRulesLearning_Apriori\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
summary(dataset)
#plot items frequency
itemFrequencyPlot(dataset, topN=10)
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.003, confidence = 0.4))
inspect(sort(rules, by = 'lift')[1:10])
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.003, confidence = 0.2))
inspect(sort(rules, by = 'lift')[1:10])
inspect(sort(rules, by = list('lift','confidence'))[1:10])
inspect(sort(rules, by = 'lift')[1:10])
inspect(sort(rules, by = c('lift','confidence'))[1:10])
inspect(sort(rules, by = 'lift')[1:10])
#training apriori
rules = apriori(data = dataset, parameter = list(support = 0.004, confidence = 0.2))
#visualization of rules
inspect(sort(rules, by = 'lift')[1:10])
dataset = read.transactions("..\\AssociationRulesLearning_Eclat\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
library(arules)
dataset = read.transactions("..\\AssociationRulesLearning_Eclat\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = FALSE)
dataset = read.transactions("..\\AssociationRulesLearning_Eclat\\Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN=10)
rules = eclat(data = dataset, parameter = list(support = 0.004, minlen = 2))
rules
inspect(sort(rules, by = 'support')[1:10])
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv.csv", header = TRUE)
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = TRUE)
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = TRUE)
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = FALSE)
size(dataset)
len
length(dataset)
count(dataset)
dataset.nrow
nrow(dataset)
a = 0
a
a = integer(0)
a
z = 1e400 z
z
2^2
10^400
aa = integer(0)
aa[1] += 1
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = FALSE)
#implement UCB
N = nrow(dataset) - 1 #remove the header row from the count
d = length(dataset) #how many ads
ads_selected = integer(0)
count_of_selection = integer(d)
sums_of_rewards = 0
reward = 0
total_reward = 0
for (n in 1:N) {
    max_upper_bound = 0
    ad = 0
    for (i in 1:d) {
        upper_bound = 0
        if (count_of_selection[i] > 0) {
            average_reward = sums_of_rewards[i] / count_of_selection[i]
            delta_i = sqrt(3 / 2 * log(n) / count_of_selection[i])
            upper_bound = average_reward + delta_i
        } else {
            upper_bound =  10^400
        }
        if (upper_bound > max_upper_bound) {
            max_upper_bound = upper_bound
            ad = i
        }
    }
    ads_selected = append(ads_selected, ad)
    count_of_selection[ad] = count_of_selection[ad] + 1
    reward = dataset[n, ad]
    sums_of_rewards[ad] = sums_of_rewards[ad] + reward
    total_reward = total_reward + reward
}
dataset = read.csv('Ads_CTR_Optimisation.csv')
dataset = read.csv("..\\ReinforcementLearning_UpperConfidenceBound\\Ads_CTR_Optimisation.csv", header = FALSE)
#implement UCB
N = nrow(dataset) - 1 #remove the header row from the count
d = length(dataset) #how many ads
ads_selected = integer(0)
count_of_selection = integer(d)
sums_of_rewards = 0
reward = 0
total_reward = 0
for (n in 1:N) {
    max_upper_bound = 0
    ad = 0
    for (i in 1:d) {
        upper_bound = 0
        if (count_of_selection[i] > 0) {
            average_reward = sums_of_rewards[i] / count_of_selection[i]
            delta_i = sqrt(3 / 2 * log(n) / count_of_selection[i])
            upper_bound = average_reward + delta_i
        } else {
            upper_bound =  1000000
        }
        if (upper_bound > max_upper_bound) {
            max_upper_bound = upper_bound
            ad = i
        }
    }
    ads_selected = append(ads_selected, ad)
    count_of_selection[ad] = count_of_selection[ad] + 1
    reward = dataset[n, ad]
    sums_of_rewards[ad] = sums_of_rewards[ad] + reward
    total_reward = total_reward + reward
}
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv", header = TRUE)
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv", header = TRUE, quote = '')
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringAsFactors = FALSE)
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus[0]
as.character(corpus[[841]])
install.packages('snowballC')
install.packages('SnowballC')
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
library(SnowballC)
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
as.character(corpus[[1]])
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers)
as.character(corpus[[1]])
corpus = tm_map(corpus, removePunctuation)
as.character(corpus[[1]])
corpus = tm_map(corpus, removeWords, stopwords())
as.character(corpus[[1]])
corpus = tm_map(corpus, stemDocument)
as.character(corpus[[1]])
cat('to lower: ', as.character(corpus[[1]]))
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]))
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
print('to lower: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeNumbers)
print('remove numbers: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removePunctuation)
print('remove punctuation:', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeWords, stopwords())
print('remove stop words: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, stemDocument)
print('stem document: ', as.character(corpus[[1]]))
print('to lower: '+ as.character(corpus[[1]]))
writeLines('to lower: '+ as.character(corpus[[1]]))
writeLines('to lower: '+ as.character(corpus
cat('stem document: ', as.character(corpus[[1]]),'\n')[[1]]))
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]))
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]))
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]),'\n')
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]),'\n')
corpus = tm_map(corpus, stripWhitespace)
cat('stem document: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, stripWhitespace)
cat('stem document: ', as.character(corpus[[841]]), '\n')
dtm = DocumentTermMatrix(corpus)
library(SnowballC)
dtm = DocumentTermMatrix(corpus)
dtm
library(SnowballC)
dtm = DocumentTermMatrix(corpus)
dtm
dtm = removeSparseTerms(dtm, 0.99)
dtm
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]),'\n')
corpus = tm_map(corpus, stripWhitespace)
cat('stem document: ', as.character(corpus[[841]]), '\n')
#install.packages('SnowballC')
# build sparse matrix
library(SnowballC)
dtm = DocumentTermMatrix(corpus)
dtm
dtm = removeSparseTerms(dtm, 0.999)
dtm
type(dtm)
dtm
# create X,y from the dataset, build dataframe
dataset = as.data.frame(as.matrix(dtm))
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
dependentVariable_Liked = dataset$Liked
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]),'\n')
corpus = tm_map(corpus, stripWhitespace)
cat('stem document: ', as.character(corpus[[841]]), '\n')
#install.packages('SnowballC')
# build sparse matrix
library(SnowballC)
dtm = DocumentTermMatrix(corpus)
dtm
#minimize matrix
dtm = removeSparseTerms(dtm, 0.999)
dtm
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dependentVariable_Liked
dataset$Liked
dataset = read.delim("..\\NaturalLanguageProcessing\\Restaurant_Reviews.tsv",
    header = TRUE, quote = '', stringsAsFactors = FALSE)
#cleaning the text
#install.packages('tm')
dependentVariable_Liked = dataset$Liked
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower))
cat('to lower: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeNumbers)
cat('remove numbers: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removePunctuation)
cat('remove punctuation:', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, removeWords, stopwords())
cat('remove stop words: ', as.character(corpus[[1]]), '\n')
corpus = tm_map(corpus, stemDocument)
cat('stem document: ', as.character(corpus[[1]]),'\n')
corpus = tm_map(corpus, stripWhitespace)
cat('stem document: ', as.character(corpus[[841]]), '\n')
#install.packages('SnowballC')
# build sparse matrix
library(SnowballC)
dtm = DocumentTermMatrix(corpus)
dtm
#minimize matrix
dtm = removeSparseTerms(dtm, 0.999)
dtm
# create X,y from the dataset, build dataframe
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dependentVariable_Liked
# Encoding the target feature as factor dataset$Liked = factor(dataset$Liked, levels = c(0, 1)) dataset
# Splitting the dataset into the Training set and Test set # install.packages('caTools') library(caTools) set.seed(123) split = sample.split(dataset$Liked, SplitRatio = 0.8) training_set = subset(dataset, split == TRUE) test_set = subset(dataset, split == FALSE)
# Fitting classifier to the Training set # Create your classifier here library(randomForest) classifier = randomForest(x = training_set[-692],                           y = training_set$Liked,                           ntree = 10) # Predicting the Test set results y_pred = predict(classifier, newdata = test_set[-692]) y_pred test_set$Liked # Making the Confusion Matrix # cm = table(test_set[, 3], y_pred) cm = table(test_set$Liked, y_pred) cm
